{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "## Spark\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Apache_Spark_logo.svg/1280px-Apache_Spark_logo.svg.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "taxi = spark.read.csv('s3://nyc-tlc/trip data/yellow_tripdata_2019-01.csv',\n",
    "                      header=True,\n",
    "                      inferSchema=True,\n",
    "                      timestampFormat='yyyy-MM-dd HH:mm:ss',\n",
    "                    ).sample(fraction=0.1, withReplacement=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, tpep_pickup_datetime: timestamp, tpep_dropoff_datetime: timestamp, passenger_count: double, trip_distance: double, RatecodeID: int, store_and_fwd_flag: string, PULocationID: int, DOLocationID: int, payment_type: int, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, congestion_surcharge: double, pickup_weekday: double, pickup_weekofyear: double, pickup_hour: double, pickup_minute: double, pickup_year_seconds: double, pickup_week_hour: double, label: double]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi = taxi.withColumn('pickup_weekday', F.dayofweek(taxi.tpep_pickup_datetime).cast(T.DoubleType()))\n",
    "taxi = taxi.withColumn('pickup_weekofyear', F.weekofyear(taxi.tpep_pickup_datetime).cast(T.DoubleType()))\n",
    "taxi = taxi.withColumn('pickup_hour', F.hour(taxi.tpep_pickup_datetime).cast(T.DoubleType()))\n",
    "taxi = taxi.withColumn('pickup_minute', F.minute(taxi.tpep_pickup_datetime).cast(T.DoubleType()))\n",
    "taxi = taxi.withColumn('pickup_year_seconds', \n",
    "                                 (F.unix_timestamp(taxi.tpep_pickup_datetime) -\n",
    "                                  F.unix_timestamp(\n",
    "                                      F.lit(datetime.datetime(2019, 1, 1, 0, 0, 0)))).cast(T.DoubleType()))\n",
    "taxi = taxi.withColumn('pickup_week_hour', ((taxi.pickup_weekday * 24) + taxi.pickup_hour).cast(T.DoubleType()))\n",
    "taxi = taxi.withColumn('passenger_count', F.coalesce(taxi.passenger_count, F.lit(-1)).cast(T.DoubleType()))\n",
    "taxi = taxi.fillna({'VendorID': 'missing', 'RatecodeID': 'missing', 'store_and_fwd_flag': 'missing' })\n",
    "# Spark ML expects a \"label\" column for the dependent variable\n",
    "taxi = taxi.withColumn('label', taxi.total_amount)\n",
    "\n",
    "taxi.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "numeric_feat = ['pickup_weekday',  'pickup_weekofyear', 'pickup_hour', 'pickup_minute',\n",
    "                'pickup_year_seconds', 'pickup_week_hour',  'passenger_count']\n",
    "categorical_feat = ['VendorID', 'RatecodeID', 'store_and_fwd_flag',\n",
    "                    'PULocationID', 'DOLocationID']\n",
    "features = numeric_feat + categorical_feat\n",
    "y_col = 'total_amount'\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(\n",
    "    inputCol=c, \n",
    "    outputCol=f'{c}_idx', handleInvalid='keep')\n",
    "    for c in categorical_feat\n",
    "]\n",
    "encoders = [\n",
    "    OneHotEncoder(\n",
    "        inputCol=f'{c}_idx',\n",
    "        outputCol=f'{c}_onehot',\n",
    "    ) \n",
    "    for c in categorical_feat\n",
    "]\n",
    "num_assembler = VectorAssembler(\n",
    "    inputCols=numeric_feat,\n",
    "    outputCol='num_features',\n",
    ")\n",
    "scaler = StandardScaler(inputCol='num_features', outputCol='num_features_scaled')\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f'{c}_onehot' for c in categorical_feat] + ['num_features_scaled'],\n",
    "    outputCol='features',\n",
    ")\n",
    "\n",
    "lr = LinearRegression(standardization=False, maxIter=100)\n",
    "pipeline = Pipeline(\n",
    "    stages=indexers + encoders + [num_assembler, scaler, assembler, lr])\n",
    "\n",
    "grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(lr.elasticNetParam, np.arange(0, 1.01, 0.01))\n",
    "    .addGrid(lr.regParam, [0, 0.5, 1, 2])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=grid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 23s, sys: 39.8 s, total: 3min 3s\n",
      "Wall time: 54min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fitted = crossval.fit(taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale up to 10 nodes\n",
    "\n",
    "(how depends on where Spark cluster is running, need to restart kernal and run all cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 34s, sys: 41.6 s, total: 3min 16s\n",
      "Wall time: 49min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fitted = crossval.fit(taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale up to 20 nodes\n",
    "\n",
    "(how depends on where Spark cluster is running, need to restart kernal and run all cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 44s, sys: 44.1 s, total: 3min 28s\n",
      "Wall time: 47min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fitted = crossval.fit(taxi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
