{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "## Spark\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Apache_Spark_logo.svg/1280px-Apache_Spark_logo.svg.png\" width=\"400\">\n",
    "\n",
    "**Hardware**: 20 nodes, r5.2xlarge (8 CPU, 64 GB RAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config('spark.executor.memory', '36g')\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import functools\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually specify schema because inferSchema in read.csv is quite slow\n",
    "schema = StructType([\n",
    "    StructField('VendorID', DoubleType()),\n",
    "    StructField('tpep_pickup_datetime', TimestampType()),\n",
    "    StructField('tpep_dropoff_datetime', TimestampType()),\n",
    "    StructField('passenger_count', DoubleType()),\n",
    "    StructField('trip_distance', DoubleType()),\n",
    "    StructField('RatecodeID', DoubleType()),\n",
    "    StructField('store_and_fwd_flag', StringType()),\n",
    "    StructField('PULocationID', DoubleType()),\n",
    "    StructField('DOLocationID', DoubleType()),\n",
    "    StructField('payment_type', DoubleType()),\n",
    "    StructField('fare_amount', DoubleType()),\n",
    "    StructField('extra', DoubleType()),\n",
    "    StructField('mta_tax', DoubleType()),\n",
    "    StructField('tip_amount', DoubleType()),\n",
    "    StructField('tolls_amount', DoubleType()),\n",
    "    StructField('improvement_surcharge', DoubleType()),\n",
    "    StructField('total_amount', DoubleType()),\n",
    "    StructField('congestion_surcharge', DoubleType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "files = [f\"s3://{x}\" for x in fs.ls('s3://nyc-tlc/trip data/')\n",
    "         if 'yellow' in x and ('2019' in x or '2018' in x or '2017' in x)]\n",
    "cols = ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance',\n",
    "        'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount',\n",
    "        'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount']\n",
    "\n",
    "def read_csv(path):\n",
    "    df = spark.read.csv(path,\n",
    "                        header=True,\n",
    "                        schema=schema,\n",
    "                        timestampFormat='yyyy-MM-dd HH:mm:ss',\n",
    "                       )\n",
    "\n",
    "    df = df.select(cols)\n",
    "\n",
    "    return df\n",
    "\n",
    "dfs = []\n",
    "for tf in files:\n",
    "    df = read_csv(tf)\n",
    "    dfs.append(df)\n",
    "taxi = functools.reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 436 µs, sys: 3.64 ms, total: 4.07 ms\n",
      "Wall time: 20.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300700143"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "taxi.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = taxi.withColumn('pickup_weekday', F.dayofweek(taxi.tpep_pickup_datetime).cast(DoubleType()))\n",
    "taxi = taxi.withColumn('pickup_hour', F.hour(taxi.tpep_pickup_datetime).cast(DoubleType()))\n",
    "taxi = taxi.withColumn('pickup_minute', F.minute(taxi.tpep_pickup_datetime).cast(DoubleType()))\n",
    "taxi = taxi.withColumn('pickup_week_hour', ((taxi.pickup_weekday * 24) + taxi.pickup_hour).cast(DoubleType()))\n",
    "taxi = taxi.withColumn('store_and_fwd_flag', F.when(taxi.store_and_fwd_flag == 'Y', 1).otherwise(0))\n",
    "# Spark ML expects \"label\" column for dependent variable\n",
    "taxi = taxi.withColumn('label', taxi.total_amount)  \n",
    "taxi = taxi.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "features = ['pickup_weekday', 'pickup_hour', 'pickup_minute',\n",
    "            'pickup_week_hour', 'passenger_count', 'VendorID', \n",
    "            'RatecodeID', 'store_and_fwd_flag', 'PULocationID', \n",
    "            'DOLocationID']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features,\n",
    "    outputCol='features',\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 ms, sys: 440 µs, total: 10.8 ms\n",
      "Wall time: 54.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300700143"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "assembler_fitted = pipeline.fit(taxi)\n",
    "X = assembler_fitted.transform(taxi)\n",
    "\n",
    "X.cache()\n",
    "X.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train random forest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(numTrees=100, maxDepth=10, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 255 ms, sys: 46.2 ms, total: 301 ms\n",
      "Wall time: 36min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fitted = rf.fit(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
